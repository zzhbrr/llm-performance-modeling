# llm-performance-modeling

Use roofline model to predict distributed LLMs inference performance (latency, throughput, memory usage and so on).

Change 'workload spec', 'hardware spec', 'parallelization spec' and 'model spec' in codes.

`python modeling_with_parallel.py`

`python modeling_moe.py`